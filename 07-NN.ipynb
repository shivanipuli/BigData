{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5c21da",
   "metadata": {},
   "source": [
    "# Toy 2-layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5502ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup codes\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from builtins import range\n",
    "from builtins import object\n",
    "\n",
    "from past.builtins import xrange #pip install future # in terminal\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87abe23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install future "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc1e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "    \"\"\"\n",
    "    A two-layer fully-connected neural network. The net has an input dimension of\n",
    "    N, a hidden layer dimension of H, and performs classification over C classes.\n",
    "    We train the network with a softmax loss function and L2 regularization on the\n",
    "    weight matrices. The network uses a ReLU nonlinearity after the first fully\n",
    "    connected layer.\n",
    "\n",
    "    In other words, the network has the following architecture:\n",
    "\n",
    "    input - fully connected layer - ReLU - fully connected layer - softmax\n",
    "\n",
    "    The outputs of the second fully-connected layer are the scores for each class.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Initialize the NN\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the model. Weights are initialized to small random values and\n",
    "        biases are initialized to zero. Weights and biases are stored in the\n",
    "        variable self.params, which is a dictionary with the following keys:\n",
    "\n",
    "        W1: First layer weights; Shape (D, H)\n",
    "        b1: First layer biases; Shape (H,)\n",
    "        W2: Second layer weights; Shape (H, C)\n",
    "        b2: Second layer biases; Shape (C,)\n",
    "\n",
    "        Inputs:\n",
    "        - input_size: The dimension D of the input data.\n",
    "        - hidden_size: The number of neurons H in the hidden layer.\n",
    "        - output_size: The number of classes C.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        \"\"\"\n",
    "        Compute the loss and gradients for a two layer fully connected neural\n",
    "        network.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Shape (N, D). Each X[i] is a training sample.\n",
    "        - y: Vector of training labels.\n",
    "        - reg: Regularization strength.\n",
    "\n",
    "        Returns:\n",
    "        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
    "        the score for class c on input X[i].\n",
    "\n",
    "        If y is not None, instead return a tuple of:\n",
    "        - loss: Loss (data loss and regularization penalty) \n",
    "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "          with respect to the loss function; has the same keys as self.params.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N, D = X.shape\n",
    "\n",
    "        # Compute the forward pass\n",
    "        # Use Relu as the non-linearity function\n",
    "        scores = np.maximum(X.dot(W1) + b1, 0).dot(W2) + b2\n",
    "\n",
    "\n",
    "        # If the targets are not given then jump out, we're done\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        # Compute the loss\n",
    "        exp_scores = np.exp(scores)\n",
    "        vector = np.sum(exp_scores,axis=1)\n",
    "        exp_scores_norm = exp_scores/vector[:,None]\n",
    "        loss = np.sum(-np.log(exp_scores_norm[np.arange(N),np.array(y)]))/N\n",
    "        \n",
    "        # Add regularization loss\n",
    "        loss += reg * (np.sum(W1 * W1) + np.sum(W2 * W2))\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        grads = {}\n",
    "        \n",
    "        dev = np.zeros(scores.shape)\n",
    "        dev[np.arange(N),np.array(y)] = 1\n",
    "        dlossdscores = (exp_scores_norm - dev)/N   # N x C\n",
    "        \n",
    "        grads['b2'] = np.sum(dlossdscores, axis = 0)     # C x 1\n",
    "        grads['W2'] = np.maximum(X.dot(W1) + b1, 0).T.dot(dlossdscores) + 2*reg*W2          # H x C\n",
    "        grads['b1'] = np.sum(dlossdscores.dot(W2.T)*(X.dot(W1) + b1 >0),axis = 0) # H x 1\n",
    "        grads['W1'] = X.T.dot(dlossdscores.dot(W2.T)*(X.dot(W1) + b1 >0))+ 2*reg*W1   \n",
    "       \n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    def train(self, X, y, X_val, y_val,\n",
    "              learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "              reg=5e-6, num_iters=100,\n",
    "              batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this neural network using SGD.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Shape (N, D) \n",
    "        - y: Shape (N, )\n",
    "        - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
    "        - y_val: A numpy array of shape (N_val, ) giving validation labels.\n",
    "        - learning_rate: Scalar giving learning rate for optimization.\n",
    "        - learning_rate_decay: Scalar giving factor used to decay the learning rate\n",
    "          after each epoch.\n",
    "        - reg: Scalar giving regularization strength.\n",
    "        - num_iters: Number of steps to take when optimizing.\n",
    "        - batch_size: Number of training examples to use per step.\n",
    "        - verbose: boolean; if true print progress during optimization.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "        # Use SGD to optimize the parameters in self.model\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            mask  = np.random.choice(num_train, batch_size, replace=True)\n",
    "            X_batch = X[mask]\n",
    "            y_batch = y[mask]\n",
    "\n",
    "            # Compute loss and gradients using the current minibatch\n",
    "            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # Gradient descent \n",
    "            self.params['b1']  += -learning_rate*grads['b1']\n",
    "            self.params['W1']  += -learning_rate*grads['W1']\n",
    "            self.params['b2']  += -learning_rate*grads['b2']\n",
    "            self.params['W2']  += -learning_rate*grads['W2']\n",
    "\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "            # Every epoch, check train and val accuracy and decay learning rate.\n",
    "            if it % iterations_per_epoch == 0:\n",
    "                # Check accuracy\n",
    "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "\n",
    "                # Decay learning rate\n",
    "                learning_rate *= learning_rate_decay\n",
    "\n",
    "        return {\n",
    "          'loss_history': loss_history,\n",
    "          'train_acc_history': train_acc_history,\n",
    "          'val_acc_history': val_acc_history,\n",
    "        }\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this two-layer network to predict labels for\n",
    "        data points. For each data point we predict scores for each of the C\n",
    "        classes, and assign each data point to the class with the highest score.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Shape (N, D)\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: Shape (N,)\n",
    "        \"\"\"\n",
    "\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        scores  =  np.maximum(X.dot(W1) + b1, 0).dot(W2) + b2\n",
    "        y_pred  =  np.argmax(scores,axis = 1)\n",
    "\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f1bd6",
   "metadata": {},
   "source": [
    "# Toy data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5ffc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542cfccc",
   "metadata": {},
   "source": [
    "## Forward pass: compute scores\n",
    "Calculate the class scores, the loss, and the gradients on the parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59482499",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = net.loss(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52249da",
   "metadata": {},
   "source": [
    "## Forward pass: compute loss\n",
    "\n",
    "Calculate the model loss and regularizaion loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04caf115",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.05)\n",
    "correct_loss = 1.30378789133\n",
    "\n",
    "# should be very small, we get < 1e-12\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d6921",
   "metadata": {},
   "source": [
    "## Backward pass: compute gradient\n",
    "\n",
    "Compute the gradient of the loss with respect to the variables `W1`, `b1`, `W2`, and `b2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/classes/20800_winter2024/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db75a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_check import eval_numerical_gradient\n",
    "\n",
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# The difference between the numeric and analytic gradients should be less than \n",
    "# 1e-8 for each of W1, W2, b1, and b2.\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.05)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.05)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93069259",
   "metadata": {},
   "source": [
    "## Combine the above: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9910afff",
   "metadata": {},
   "source": [
    "Use SGD to train the NN. The training loss should less than 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = init_toy_model()\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, reg=5e-6,\n",
    "            num_iters=100, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration',size = 20)\n",
    "plt.ylabel('training loss',size = 20)\n",
    "plt.title('Training Loss history', size = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b55ec",
   "metadata": {},
   "source": [
    "# CIFAR10 Classfication\n",
    "\n",
    "Now we use CIFAR10 as the real data example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data_toy(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = '/classes/20800_winter2024/Data/cifar-10-data'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8241825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data_toy()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187e666",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Method: SGD with exponential decay learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1132fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=1000, batch_size=200,\n",
    "            learning_rate=1e-4, learning_rate_decay=0.95,\n",
    "            reg=0.25, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a7669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.figure(figsize = (15,12))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history', size = 20)\n",
    "plt.xlabel('Iteration', size = 20)\n",
    "plt.ylabel('Loss',size = 20)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history', size = 20)\n",
    "plt.xlabel('Epoch', size = 20)\n",
    "plt.ylabel('Clasification accuracy', size = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2f61e",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d15b5",
   "metadata": {},
   "source": [
    "\n",
    "**What's wrong?**. \n",
    "\n",
    "The loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. Moreover, there is no gap between the training and validation accuracy, suggesting that the model has low capacity, and that we should increase its size. \n",
    "\n",
    "On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d2625a",
   "metadata": {},
   "source": [
    "\n",
    "**Tuning**. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including \n",
    "+ hidden layer size, \n",
    "+ learning rate, \n",
    "+ numer of training epochs,\n",
    "+ regularization strength;\n",
    "+ learning rate decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa88118",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net = None # store the best model into this \n",
    "\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 75\n",
    "num_classes = 10\n",
    "results = {}\n",
    "best_val = -1\n",
    "\n",
    "learning_rates = [5e-4,1e-3]\n",
    "regularization_strengths = [0.05,0.25]\n",
    "\n",
    "\n",
    "hyper_params = [(lr, reg) for j, reg  in enumerate(regularization_strengths) for i, lr in enumerate(learning_rates)]\n",
    "\n",
    "for lr, reg_s in hyper_params:\n",
    "    \n",
    "    \n",
    "    net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "    stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=10000, batch_size=400,\n",
    "            learning_rate=lr, learning_rate_decay=0.95,\n",
    "            reg=reg_s, verbose=False)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(stats['loss_history'])\n",
    "    plt.title('Loss history')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(stats['train_acc_history'], label='train')\n",
    "    plt.plot(stats['val_acc_history'], label='val')\n",
    "    plt.title('Classification accuracy history')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Classification accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "   \n",
    "    y_train_pred = net.predict(X_train)\n",
    "    y_val_pred = net.predict(X_val)\n",
    "    results.update({(lr, reg_s): (np.mean(y_train == y_train_pred),np.mean(y_val == y_val_pred))})\n",
    "    new_val  =  np.mean(y_val == y_val_pred)\n",
    "    if new_val>best_val:\n",
    "        best_val = new_val\n",
    "        best_net = net\n",
    "        print('lr %e reg %e val accuracy: %f' % (lr, reg_s, new_val))\n",
    "\n",
    "\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd5680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis_utils import visualize_grid\n",
    "\n",
    "def show_net_weights(net):\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# visualize the weights of the best network\n",
    "show_net_weights(best_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc3700",
   "metadata": {},
   "source": [
    "#  Multi-layer NN\n",
    "\n",
    "Now we have finished the top layer, and we want to build networks using a more modular design so that we can implement different layer types in isolation and then snap them together into models with different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775cdcf3",
   "metadata": {},
   "source": [
    "For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive dout (derivative of loss with respect to outputs) and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c2f678",
   "metadata": {},
   "source": [
    "## Affine layer: foward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67649377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). \n",
    "    Reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    \n",
    "    out = np.reshape(x,(x.shape[0],np.prod(x.shape[1:]))).dot(w) + b\n",
    "    \n",
    "\n",
    "    cache = (x, w, b)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c7df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around e-9 or less.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31cdc59",
   "metadata": {},
   "source": [
    "## Affine layer: backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238fedbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data, of shape (N, d_1, ... d_k)\n",
    "      - w: Weights, of shape (D, M)\n",
    "      - b: Biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "\n",
    "    db = np.sum(dout, axis = 0)   \n",
    "    dw = np.reshape(x,(x.shape[0],np.prod(x.shape[1:]))).T.dot(dout)\n",
    "    dx = dout.dot(w.T).reshape(x.shape) \n",
    "    \n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81431175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "\n",
    "\n",
    "# Test the affine_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around e-10 or less\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa25332",
   "metadata": {},
   "source": [
    "##  ReLU activation: forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95752a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "    \"\"\"\n",
    "    out = np.maximum(x, 0)\n",
    "    cache = x\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45352122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdfd086",
   "metadata": {},
   "source": [
    "## ReLU activation: backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ee54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of ReLUs.\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "\n",
    "    dx = dout*(x>0)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5254bb71",
   "metadata": {},
   "source": [
    "## Loss layers: Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
    "      class for the ith input.\n",
    "    - y: Vector of labels, of shape (N,)\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    \n",
    "    shifted_logits = x - np.max(x, axis=1, keepdims=True)\n",
    "    Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)\n",
    "    log_probs = shifted_logits - np.log(Z)\n",
    "    probs = np.exp(log_probs)\n",
    "    N = x.shape[0]\n",
    "    loss = -np.sum(log_probs[np.arange(N), y]) / N\n",
    "    dx = probs.copy()\n",
    "    dx[np.arange(N), y] -= 1\n",
    "    dx /= N\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be close to 2.3 and dx error should be around e-8\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe32c12",
   "metadata": {},
   "source": [
    "# Multilayer network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1482b1",
   "metadata": {},
   "source": [
    "## Initial loss and gradient check\n",
    "\n",
    "As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. Do the initial losses seem reasonable?\n",
    "\n",
    "For gradient checking, you should expect to see errors around 1e-7 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59761bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNet(object):\n",
    "    \"\"\"\n",
    "    A fully-connected neural network with an arbitrary number of hidden layers,\n",
    "    ReLU nonlinearities, and a softmax loss function. For a network with L layers,\n",
    "    the architecture will be\n",
    "\n",
    "    {affine - [batch/layer norm] - relu - [dropout]} x (L - 1) - affine - softmax\n",
    "\n",
    "    where batch/layer normalization and dropout are optional, and the {...} block is\n",
    "    repeated L - 1 times.\n",
    "\n",
    "    Learnable parameters are stored in the\n",
    "    self.params dictionary and will be learned using the Solver class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dims, input_dim=3*32*32, num_classes=10,\n",
    "                 reg=0.0,\n",
    "                 weight_scale=1e-2, dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Initialize a new FullyConnectedNet.\n",
    "\n",
    "        Inputs:\n",
    "        - hidden_dims: A list of integers giving the size of each hidden layer.\n",
    "        - input_dim: An integer giving the size of the input.\n",
    "        - num_classes: An integer giving the number of classes to classify.\n",
    "        - reg: Scalar giving L2 regularization strength.\n",
    "        - weight_scale: Scalar giving the standard deviation for random\n",
    "          initialization of the weights.\n",
    "        - dtype: A numpy datatype object; all computations will be performed using\n",
    "          this datatype. float32 is faster but less accurate, so you should use\n",
    "          float64 for numeric gradient checking.\n",
    "\n",
    "        \"\"\"\n",
    "        self.reg = reg\n",
    "        self.num_layers = 1 + len(hidden_dims)\n",
    "        self.dtype = dtype\n",
    "        self.params = {}\n",
    "\n",
    "        if len(hidden_dims)>0:\n",
    "            self.params['W1'] = weight_scale * np.random.randn(input_dim, hidden_dims[0])\n",
    "            self.params['b1'] = np.zeros(hidden_dims[0])\n",
    "        \n",
    "        for idx, dim in enumerate(hidden_dims):\n",
    "            if idx < len(hidden_dims)-1:\n",
    "                self.params['W' + str(2 + idx)] = weight_scale * np.random.randn(dim, hidden_dims[idx + 1])\n",
    "                self.params['b' + str(2 + idx)] = np.zeros(hidden_dims[idx + 1])\n",
    "            else:\n",
    "                self.params['W' + str(2 + idx)] = weight_scale * np.random.randn(hidden_dims[idx], num_classes)\n",
    "                self.params['b' + str(2 + idx)] = np.zeros(num_classes)\n",
    "\n",
    "\n",
    "        # Cast all parameters to the correct datatype\n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.astype(dtype)\n",
    "\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient for the fully-connected net.\n",
    "\n",
    "        Input / output: Same as TwoLayerNet above.\n",
    "        \"\"\"\n",
    "        X = X.astype(self.dtype)\n",
    "        mode = 'test' if y is None else 'train'\n",
    "\n",
    "        out = X\n",
    "        for la in range(self.num_layers-1):\n",
    "            a, _      = affine_forward(out, self.params['W'+str(la+1)], self.params['b'+str(la+1)])\n",
    "            out, _    = relu_forward(a)\n",
    "            \n",
    "\n",
    "            \n",
    "        scores, _ = affine_forward(out,\n",
    "                                   self.params['W'+str(self.num_layers)],self.params['b'+str(self.num_layers)])\n",
    "\n",
    "\n",
    "        # If test mode return early\n",
    "        if mode == 'test':\n",
    "            return scores\n",
    "\n",
    "        out = X\n",
    "        a=[]\n",
    "        affine_cache = []\n",
    "        relu_cache   = []\n",
    "       \n",
    "            \n",
    "        for la in range(self.num_layers-1):\n",
    "            a, a_cache    = affine_forward(out, self.params['W'+str(la+1)], self.params['b'+str(la+1)])\n",
    "            affine_cache.append(a_cache)\n",
    "                \n",
    "            out, r_cache    = relu_forward(a)\n",
    "            relu_cache.append(r_cache)\n",
    "            \n",
    "   \n",
    "        scores, cache  = affine_forward(out, self.params['W'+str(self.num_layers)], self.params['b'+str(self.num_layers)])\n",
    "        loss, dx       = softmax_loss(scores, y)\n",
    "        \n",
    "        grads = {}\n",
    "    \n",
    "        da, dW, db  = affine_backward(dx, cache)\n",
    "        grads['W'+str(self.num_layers)] = dW\n",
    "        grads['b'+str(self.num_layers)] = db\n",
    "            \n",
    "        for la in reversed(range(self.num_layers-1)):            \n",
    "            \n",
    "            dr = relu_backward(da, relu_cache[la])\n",
    "            \n",
    "            da, dW, db = affine_backward(dr, affine_cache[la])\n",
    "            grads['W'+ str(la+1)] = dW\n",
    "            grads['b'+ str(la+1)] = db\n",
    "            \n",
    "        for la in range(self.num_layers):\n",
    "            loss += 1/2*self.reg * np.sum(self.params['W'+ str(la+1)] * self.params['W'+ str(la+1)])     \n",
    "            grads['W'+ str(la+1)]  +=  self.reg*self.params['W'+ str(la+1)] \n",
    "\n",
    "        return loss, grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ce2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print('Running check with reg = ', reg)\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: ', loss)\n",
    "  \n",
    "  # Most of the errors should be on the order of e-7 or smaller.   \n",
    "  # NOTE: It is fine however to see an error for W2 on the order of e-5\n",
    "  # for the check when reg = 0.0\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be041668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use a five-layer Net to overfit 50 training examples by \n",
    "# tweaking just the learning rate and initialization scale.\n",
    "from data_utils import get_CIFAR10_data\n",
    "\n",
    "data = get_CIFAR10_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d575af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from solver import Solver\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "learning_rate = 2e-3  # Experiment with this!\n",
    "weight_scale = 1*1e-1   # Experiment with this!\n",
    "model = FullyConnectedNet([100, 100, 100, 100],\n",
    "                weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdde380",
   "metadata": {},
   "source": [
    "# SGD+Momentum\n",
    "Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochastic gradient descent.\n",
    "\n",
    "Run the following to check your implementation. You should see errors less than e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38395739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optim import sgd_momentum\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-3, 'velocity': v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "# Should see relative errors around e-8 or less\n",
    "print('next_w error: ', rel_error(next_w, expected_next_w))\n",
    "print('velocity error: ', rel_error(expected_velocity, config['velocity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24954e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in ['sgd', 'sgd_momentum']:\n",
    "  print('running with ', update_rule)\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': 5e-3,\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=\"loss_%s\" % update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=\"train_acc_%s\" % update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=\"val_acc_%s\" % update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6308db85",
   "metadata": {},
   "source": [
    "# Train a good model!\n",
    "Train the best fully-connected model that you can on CIFAR-10, storing your best model in the `best_model` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2117785",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "\n",
    "weight_scale = 10*2*1e-4   # Experiment with this!\n",
    "learning_rate = 9*1e-3  # Experiment with this!\n",
    "model = FullyConnectedNet([100,100,100], reg = 1*1e-2,\n",
    "              weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, data,\n",
    "                print_every=100, num_epochs=20, batch_size=500, \n",
    "                update_rule='sgd_momentum',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "best_model = solver.model\n",
    "\n",
    "plt.figure(figsize = (12,12))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ae0373",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dddfa1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb077b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
